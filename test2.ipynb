{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce code : Les données AWAY et HOME sont soustraites pour n'avoir que des différences. De ce fait, toutes les variables dans ces modèles sont \"des nouvelles variables\". Cette technique a été choisie car dans les modèles précédents, les variables les plus importantes ont été celles qui était des différences.\n",
    "(Variables trop corrélées et gestion des NA grâce a al ligue quelquesoit le code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#!pip install --upgrade --force-reinstall matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#!pip install tqdm \n",
    "#!pip install xgboost\n",
    "from tqdm import tqdm\n",
    "#!pip install scikit-learn\n",
    "#!pip install xgboost\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install optuna\n",
    "import optuna\n",
    "#!pip install catboost\n",
    "from catboost import CatBoostClassifier\n",
    "#!pip install pycaret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_home_team = pd.read_csv('C:/Users/agraf/Desktop/uni/master/volume 2 foot (1)/volume 2 foot/train_home_team_statistics_df.csv', index_col=0)\n",
    "train_away_team = pd.read_csv('C:/Users/agraf/Desktop/uni/master/volume 2 foot (1)/volume 2 foot/train_away_team_statistics_df.csv', index_col=0)\n",
    "\n",
    "train_home_player = pd.read_csv('C:/Users/agraf/Desktop/uni/master/volume 2 foot (1)/volume 2 foot/train_home_player_statistics_df.csv', index_col=0)\n",
    "train_away_player = pd.read_csv('C:/Users/agraf/Desktop/uni/master/volume 2 foot (1)/volume 2 foot/train_away_player_statistics_df.csv', index_col=0)\n",
    "\n",
    "train_scores = pd.read_csv('C:/Users/agraf/Desktop/uni/master/volume 2 foot (1)/volume 2 foot/Y_train.csv', index_col=0)\n",
    "\n",
    "test_home_team = pd.read_csv('C:/Users/agraf/Desktop/uni/master/volume 2 foot (1)/volume 2 foot/test_home_team_statistics_df.csv', index_col=0)\n",
    "test_away_team = pd.read_csv('C:/Users/agraf/Desktop/uni/master/volume 2 foot (1)/volume 2 foot/test_away_team_statistics_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ligue 1' 'Ligue 2' 'Serie A' 'League One' 'Premier League'\n",
      " 'Liga Portugal' 'La Liga' 'Superliga' 'Bundesliga' 'Pro League'\n",
      " 'J-League' 'League Two' 'Eredivisie']\n",
      "               LEAGUE  TEAM_SHOTS_TOTAL_season_sum  \\\n",
      "ID                                                   \n",
      "0             Ligue 1                          3.0   \n",
      "1             Ligue 2                          6.0   \n",
      "2             Serie A                          4.0   \n",
      "3          League One                          7.0   \n",
      "4      Premier League                          3.0   \n",
      "...               ...                          ...   \n",
      "12298      League One                          4.0   \n",
      "12299   Liga Portugal                          4.0   \n",
      "12300      Bundesliga                          4.0   \n",
      "12301      League One                          2.0   \n",
      "12302         La Liga                          2.0   \n",
      "\n",
      "       TEAM_SHOTS_INSIDEBOX_season_sum  TEAM_SHOTS_OFF_TARGET_season_sum  \\\n",
      "ID                                                                         \n",
      "0                             2.000000                               5.0   \n",
      "1                             8.000000                               3.0   \n",
      "2                             2.000000                               5.0   \n",
      "3                             5.000000                               5.0   \n",
      "4                             3.000000                               2.0   \n",
      "...                                ...                               ...   \n",
      "12298                         2.000000                               3.0   \n",
      "12299                         2.000000                               3.0   \n",
      "12300                         3.000000                               5.0   \n",
      "12301                         5.365714                               1.0   \n",
      "12302                         3.000000                               1.0   \n",
      "\n",
      "       TEAM_SHOTS_ON_TARGET_season_sum  TEAM_SHOTS_OUTSIDEBOX_season_sum  \\\n",
      "ID                                                                         \n",
      "0                                  2.0                          1.000000   \n",
      "1                                  6.0                          5.000000   \n",
      "2                                  2.0                          8.000000   \n",
      "3                                  6.0                          6.000000   \n",
      "4                                  3.0                          4.000000   \n",
      "...                                ...                               ...   \n",
      "12298                              4.0                          7.000000   \n",
      "12299                              1.0                          5.000000   \n",
      "12300                              3.0                          5.000000   \n",
      "12301                              1.0                          4.602857   \n",
      "12302                              2.0                          2.000000   \n",
      "\n",
      "       TEAM_PASSES_season_sum  TEAM_SUCCESSFUL_PASSES_season_sum  \\\n",
      "ID                                                                 \n",
      "0                    2.000000                           2.000000   \n",
      "1                    8.000000                           7.000000   \n",
      "2                    1.000000                           1.000000   \n",
      "3                    9.000000                           9.000000   \n",
      "4                    4.000000                           3.000000   \n",
      "...                       ...                                ...   \n",
      "12298                4.000000                           4.000000   \n",
      "12299                1.000000                           1.000000   \n",
      "12300                1.000000                           1.000000   \n",
      "12301                4.928571                           4.608571   \n",
      "12302                3.000000                           4.000000   \n",
      "\n",
      "       TEAM_SAVES_season_sum  TEAM_CORNERS_season_sum  ...  \\\n",
      "ID                                                     ...   \n",
      "0                        5.0                      3.0  ...   \n",
      "1                       10.0                      6.0  ...   \n",
      "2                        2.0                      2.0  ...   \n",
      "3                        2.0                      2.0  ...   \n",
      "4                        4.0                      4.0  ...   \n",
      "...                      ...                      ...  ...   \n",
      "12298                    3.0                      2.0  ...   \n",
      "12299                    9.0                      1.0  ...   \n",
      "12300                    6.0                      1.0  ...   \n",
      "12301                    0.0                      4.0  ...   \n",
      "12302                    7.0                      4.0  ...   \n",
      "\n",
      "       TEAM_YELLOWCARDS_5_last_match_std  TEAM_REDCARDS_5_last_match_std  \\\n",
      "ID                                                                         \n",
      "0                                    3.0                             0.0   \n",
      "1                                    4.0                             0.0   \n",
      "2                                    4.0                             5.0   \n",
      "3                                    4.0                             0.0   \n",
      "4                                    1.0                             0.0   \n",
      "...                                  ...                             ...   \n",
      "12298                                4.0                             0.0   \n",
      "12299                                6.0                            10.0   \n",
      "12300                                7.0                             8.0   \n",
      "12301                               10.0                            10.0   \n",
      "12302                                3.0                             5.0   \n",
      "\n",
      "       TEAM_OFFSIDES_5_last_match_std  TEAM_ATTACKS_5_last_match_std  \\\n",
      "ID                                                                     \n",
      "0                             6.00000                            0.0   \n",
      "1                             4.00000                            3.0   \n",
      "2                             6.00000                            3.0   \n",
      "3                             1.00000                            8.0   \n",
      "4                             2.00000                            5.0   \n",
      "...                               ...                            ...   \n",
      "12298                         2.00000                            5.0   \n",
      "12299                         7.00000                            2.0   \n",
      "12300                         0.00000                            7.0   \n",
      "12301                         3.69382                            6.0   \n",
      "12302                         2.00000                            1.0   \n",
      "\n",
      "       TEAM_PENALTIES_5_last_match_std  TEAM_SUBSTITUTIONS_5_last_match_std  \\\n",
      "ID                                                                            \n",
      "0                                 10.0                                  8.0   \n",
      "1                                 10.0                                  0.0   \n",
      "2                                  6.0                                  7.0   \n",
      "3                                  8.0                                  5.0   \n",
      "4                                  8.0                                  7.0   \n",
      "...                                ...                                  ...   \n",
      "12298                              0.0                                  5.0   \n",
      "12299                              0.0                                  0.0   \n",
      "12300                              0.0                                  0.0   \n",
      "12301                              8.0                                  3.0   \n",
      "12302                              0.0                                  8.0   \n",
      "\n",
      "       TEAM_BALL_SAFE_5_last_match_std  \\\n",
      "ID                                       \n",
      "0                                  7.0   \n",
      "1                                  1.0   \n",
      "2                                  2.0   \n",
      "3                                  5.0   \n",
      "4                                  2.0   \n",
      "...                                ...   \n",
      "12298                              0.0   \n",
      "12299                              3.0   \n",
      "12300                              3.0   \n",
      "12301                              0.0   \n",
      "12302                              2.0   \n",
      "\n",
      "       TEAM_DANGEROUS_ATTACKS_5_last_match_std  \\\n",
      "ID                                               \n",
      "0                                          2.0   \n",
      "1                                          2.0   \n",
      "2                                          3.0   \n",
      "3                                          5.0   \n",
      "4                                          6.0   \n",
      "...                                        ...   \n",
      "12298                                      0.0   \n",
      "12299                                     10.0   \n",
      "12300                                      1.0   \n",
      "12301                                      3.0   \n",
      "12302                                      0.0   \n",
      "\n",
      "       TEAM_INJURIES_5_last_match_std  TEAM_GOALS_5_last_match_std  \n",
      "ID                                                                  \n",
      "0                            4.000000                          3.0  \n",
      "1                            8.000000                          4.0  \n",
      "2                            2.000000                          4.0  \n",
      "3                            1.333333                          6.0  \n",
      "4                            4.000000                          4.0  \n",
      "...                               ...                          ...  \n",
      "12298                        1.333333                          4.0  \n",
      "12299                        6.000000                          1.0  \n",
      "12300                        3.000000                          2.0  \n",
      "12301                        1.333333                          5.0  \n",
      "12302                        6.000000                          6.0  \n",
      "\n",
      "[12303 rows x 141 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop column: 'TEAM_NAME'\n",
    "train_home_team = train_home_team.drop(columns=['TEAM_NAME'])\n",
    "# Pour train_home_team\n",
    "train_home_team = train_home_team.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Pour train_away_team\n",
    "train_away_team = train_away_team.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Pour test_home_team\n",
    "test_home_team = test_home_team.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "# Pour test_away_team\n",
    "test_away_team = test_away_team.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "\n",
    "#Voir les différentes league :\n",
    "unique_leagues = train_home_team['LEAGUE'].unique()\n",
    "print(unique_leagues)\n",
    "\n",
    "#Calculer la moyenne de chaque colonne par ligue\n",
    "means_per_league_home = train_home_team.groupby('LEAGUE').transform('mean')\n",
    "\n",
    "# Remplacer les valeurs NA par les moyennes de la ligue correspondante\n",
    "train_home_team = train_home_team.fillna(means_per_league_home)\n",
    "\n",
    "print(train_home_team)\n",
    "\n",
    "#On enleve \"TEAM_INJURIES_5_last_match_std\" car elle contient quand meme des NA, ainsi que league car la variable ne nous sert plus\n",
    "train_home_team = train_home_team.drop(columns=['TEAM_INJURIES_5_last_match_std'])\n",
    "train_home_team = train_home_team.drop(columns=['LEAGUE'])\n",
    "\n",
    "# Pour train_away_team\n",
    "# Drop column: 'TEAM_NAME'\n",
    "train_away_team = train_away_team.drop(columns=['TEAM_NAME'])\n",
    "\n",
    "# Calculer la moyenne de chaque colonne par ligue\n",
    "means_per_league_away = train_away_team.groupby('LEAGUE').transform('mean')\n",
    "\n",
    "# Remplacer les valeurs NA par les moyennes de la ligue correspondante\n",
    "train_away_team = train_away_team.fillna(means_per_league_away)\n",
    "\n",
    "# On enlève \"TEAM_INJURIES_5_last_match_std\" et 'LEAGUE'\n",
    "train_away_team = train_away_team.drop(columns=['TEAM_INJURIES_5_last_match_std'])\n",
    "train_away_team = train_away_team.drop(columns=['LEAGUE'])\n",
    "\n",
    "# Pour test_home_team\n",
    "\n",
    "# Remplacer les valeurs NA par les moyennes correspondantes à chaque colonne des équipes home\n",
    "test_home_team = test_home_team.fillna(means_per_league_home.mean())\n",
    "\n",
    "# On enlève \"TEAM_INJURIES_5_last_match_std\"\n",
    "test_home_team = test_home_team.drop(columns=['TEAM_INJURIES_5_last_match_std'])\n",
    "\n",
    "# Pour test_away_team\n",
    "\n",
    "# Remplacer les valeurs NA par les moyennes correspondantes à chaque colonne des équipes away\n",
    "test_away_team = test_away_team.fillna(means_per_league_away.mean())\n",
    "\n",
    "# On enlève \"TEAM_INJURIES_5_last_match_std\"\n",
    "test_away_team = test_away_team.drop(columns=['TEAM_INJURIES_5_last_match_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les données d'entraînement\n",
    "# Calculer la différence entre les données d'équipe à domicile et à l'extérieur\n",
    "train_diff = train_home_team.values - train_away_team.values\n",
    "train_diff_df = pd.DataFrame(train_diff, index=train_home_team.index, columns=train_home_team.columns.str.replace('HOME_', 'DIFF_'))\n",
    "\n",
    "# Ajouter la colonne des scores\n",
    "train_scores = train_scores.loc[train_diff_df.index]\n",
    "train_diff_df = pd.concat([train_diff_df, train_scores], axis=1)\n",
    "\n",
    "# Déterminer la variable cible (target_variable)\n",
    "train_diff_df['target_variable'] = train_diff_df[['HOME_WINS', 'DRAW', 'AWAY_WINS']].idxmax(axis=1)\n",
    "\n",
    "# Mapper les valeurs cibles en valeurs numériques\n",
    "target_mapping = {'HOME_WINS': 0, 'DRAW': 1, 'AWAY_WINS': 2}\n",
    "train_diff_df['target_variable'] = train_diff_df['target_variable'].map(target_mapping)\n",
    "\n",
    "# Supprimer les colonnes des scores, car elles ne sont plus nécessaires après avoir créé la variable cible\n",
    "train_diff_df.drop(columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'], inplace=True)\n",
    "\n",
    "# Pour les données de test\n",
    "# Calculer la différence entre les données d'équipe à domicile et à l'extérieur\n",
    "test_diff = test_home_team.values - test_away_team.values\n",
    "test_diff_df = pd.DataFrame(test_diff, index=test_home_team.index, columns=test_home_team.columns.str.replace('HOME_', 'DIFF_'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de variables supprimées : 49\n",
      "Variables supprimées : ['TEAM_SUCCESSFUL_PASSES_season_sum', 'TEAM_SHOTS_TOTAL_season_average', 'TEAM_SHOTS_INSIDEBOX_season_average', 'TEAM_SHOTS_OFF_TARGET_season_average', 'TEAM_SHOTS_ON_TARGET_season_average', 'TEAM_SHOTS_OUTSIDEBOX_season_average', 'TEAM_PASSES_season_average', 'TEAM_SUCCESSFUL_PASSES_season_average', 'TEAM_SUCCESSFUL_PASSES_PERCENTAGE_season_average', 'TEAM_SAVES_season_average', 'TEAM_CORNERS_season_average', 'TEAM_FOULS_season_average', 'TEAM_YELLOWCARDS_season_average', 'TEAM_REDCARDS_season_average', 'TEAM_OFFSIDES_season_average', 'TEAM_ATTACKS_season_average', 'TEAM_PENALTIES_season_average', 'TEAM_DANGEROUS_ATTACKS_season_average', 'TEAM_GOALS_season_average', 'TEAM_GAME_WON_season_average', 'TEAM_GAME_DRAW_season_average', 'TEAM_GAME_LOST_season_average', 'TEAM_SUCCESSFUL_PASSES_season_std', 'TEAM_REDCARDS_season_std', 'TEAM_SUCCESSFUL_PASSES_5_last_match_sum', 'TEAM_SHOTS_TOTAL_5_last_match_average', 'TEAM_SHOTS_INSIDEBOX_5_last_match_average', 'TEAM_SHOTS_OFF_TARGET_5_last_match_average', 'TEAM_SHOTS_ON_TARGET_5_last_match_average', 'TEAM_SHOTS_OUTSIDEBOX_5_last_match_average', 'TEAM_PASSES_5_last_match_average', 'TEAM_SUCCESSFUL_PASSES_5_last_match_average', 'TEAM_SAVES_5_last_match_average', 'TEAM_CORNERS_5_last_match_average', 'TEAM_FOULS_5_last_match_average', 'TEAM_YELLOWCARDS_5_last_match_average', 'TEAM_REDCARDS_5_last_match_average', 'TEAM_OFFSIDES_5_last_match_average', 'TEAM_ATTACKS_5_last_match_average', 'TEAM_PENALTIES_5_last_match_average', 'TEAM_SUBSTITUTIONS_5_last_match_average', 'TEAM_DANGEROUS_ATTACKS_5_last_match_average', 'TEAM_GOALS_5_last_match_average', 'TEAM_GAME_WON_5_last_match_average', 'TEAM_GAME_DRAW_5_last_match_average', 'TEAM_GAME_LOST_5_last_match_average', 'TEAM_SUCCESSFUL_PASSES_5_last_match_std', 'TEAM_REDCARDS_5_last_match_std', 'TEAM_PENALTIES_5_last_match_std']\n"
     ]
    }
   ],
   "source": [
    "# Définir la variable cible (Y)\n",
    "target_variable = 'target_variable'\n",
    "\n",
    "# Calculer la matrice de corrélation sans la variable cible\n",
    "corr_matrix = train_diff_df.drop(columns=[target_variable]).corr().abs()\n",
    "\n",
    "# Supprimer les variables ayant une corrélation supérieure à 0.8\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "print(f'Nombre de variables supprimées : {len(to_drop)}')\n",
    "print(f'Variables supprimées : {to_drop}')\n",
    "\n",
    "# Supprimer les variables corrélées dans le DataFrame d'origine\n",
    "train_data2 = train_diff_df.drop(columns=to_drop)\n",
    "test_data2 = test_diff_df.drop(columns=to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des modèles (comme pour l'autre code, l'ordre des modèles dans le code n'est pas l'ordre réel de création)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATBOOST avec optuna pour optimiser les hyper paramètres\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Définir les données d'entraînement\n",
    "target_variable = 'target_variable' \n",
    "X_train = train_data2.drop(columns=[target_variable])\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "# Définir une fonction d'optimisation avec Optuna pour CatBoost\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_categorical('iterations', [150, 300, 500]),\n",
    "        'depth': trial.suggest_categorical('depth', [4, 8, 10, 12]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'l2_leaf_reg': trial.suggest_categorical('l2_leaf_reg', [5, 9, 12, 14]),\n",
    "        'border_count': trial.suggest_categorical('border_count', [32, 64, 128]),\n",
    "        'loss_function': 'MultiClass',\n",
    "        'random_seed': 64,\n",
    "        'verbose': 0,\n",
    "        'early_stopping_rounds': 30\n",
    "    }\n",
    "    \n",
    "    cat_model = CatBoostClassifier(**params)\n",
    "    scores = cross_val_score(cat_model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# Lancer l'optimisation avec Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Afficher les meilleurs paramètres\n",
    "print(\"Meilleurs hyperparamètres trouvés par Optuna :\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Utiliser les meilleurs paramètres trouvés pour créer le modèle final\n",
    "best_params = study.best_params\n",
    "best_cat = CatBoostClassifier(**best_params)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres trouvés\n",
    "best_cat.fit(X_train, y_train)\n",
    "\n",
    "# Prédire la variable cible pour la base test_data (les variables explicatives)\n",
    "y_pred_test = best_cat.predict(test_data2)\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "y_pred_one_hot = np.zeros((len(y_pred_test), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred_test):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (utiliser l'index de test_data comme identifiant)\n",
    "predictions_one_hot_df.insert(0, 'ID', test_data2.index)\n",
    "\n",
    "# Exporter les résultats dans un fichier CSV\n",
    "predictions_one_hot_df.to_csv('predictions_catboost_best_one_hot.csv', index=False)\n",
    "\n",
    "print(\"Prédictions one-hot enregistrées dans le fichier 'predictions_catboost_best_one_hot.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance des variables :\n",
      "                                Feature  Importance\n",
      "21            TEAM_GAME_LOST_season_sum   12.849121\n",
      "19             TEAM_GAME_WON_season_sum    9.029985\n",
      "1       TEAM_SHOTS_INSIDEBOX_season_sum    7.367896\n",
      "22  TEAM_BALL_POSSESSION_season_average    4.700247\n",
      "3       TEAM_SHOTS_ON_TARGET_season_sum    4.438789\n",
      "..                                  ...         ...\n",
      "85        TEAM_ATTACKS_5_last_match_std    0.000000\n",
      "86  TEAM_SUBSTITUTIONS_5_last_match_std    0.000000\n",
      "23    TEAM_SUBSTITUTIONS_season_average    0.000000\n",
      "35      TEAM_BALL_POSSESSION_season_std    0.000000\n",
      "60  TEAM_SUBSTITUTIONS_5_last_match_sum    0.000000\n",
      "\n",
      "[90 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# CATBOOST N2 (avec les hyperparametre d'optuna), dans le but d'obtenir les variables les moins importante dans le modèle\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Définir les données d'entraînement\n",
    "target_variable = 'target_variable' \n",
    "X_train = train_data2.drop(columns=[target_variable])\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "# Définir les hyperparamètres optimaux trouvés\n",
    "best_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'l2_leaf_reg': 15,\n",
    "    'iterations': 50,\n",
    "    'depth': 6,\n",
    "    'border_count': 128,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'random_seed': 64,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Créer le modèle CatBoost avec les meilleurs hyperparamètres\n",
    "best_cat = CatBoostClassifier(**best_params)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres trouvés\n",
    "best_cat.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les variables qui contribuent le plus au modèle\n",
    "feature_importances = best_cat.get_feature_importance()\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"Importance des variables :\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Prédire la variable cible pour la base test_data (les variables explicatives)\n",
    "y_pred_test = best_cat.predict(test_data2)\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "y_pred_one_hot = np.zeros((len(y_pred_test), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred_test):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (utiliser l'index de test_data comme identifiant)\n",
    "predictions_one_hot_df.insert(0, 'ID', test_data2.index)\n",
    "\n",
    "# # Exporter les résultats dans un fichier CSV\n",
    "# predictions_one_hot_df.to_csv('predictions_catboost_best_one_hot.csv', index=False)\n",
    "\n",
    "# print(\"Prédictions one-hot enregistrées dans le fichier 'predictions_catboost_best_one_hot.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions one-hot enregistrées dans le fichier 'predictions4_catboost_best_one_hot.csv'\n"
     ]
    }
   ],
   "source": [
    "# CATBOOST avec suppression des variables les moins importantes\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Définir les données d'entraînement\n",
    "target_variable = 'target_variable' \n",
    "X_train = train_data2.drop(columns=[target_variable])\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "# Définir les hyperparamètres optimaux trouvés\n",
    "best_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'l2_leaf_reg': 15,\n",
    "    'iterations': 50,\n",
    "    'depth': 6,\n",
    "    'border_count': 128,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'random_seed': 64,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Créer le modèle CatBoost avec les meilleurs hyperparamètres\n",
    "best_cat = CatBoostClassifier(**best_params)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres trouvés\n",
    "best_cat.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les variables qui contribuent le plus au modèle\n",
    "feature_importances = best_cat.get_feature_importance()\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Définir un seuil pour l'importance minimale (ici de 1)\n",
    "importance_threshold = 1 \n",
    "important_features = feature_importance_df[feature_importance_df['Importance'] > importance_threshold]['Feature']\n",
    "\n",
    "# Créer un nouveau DataFrame avec uniquement les caractéristiques importantes\n",
    "X_train_reduced = X_train[important_features]\n",
    "\n",
    "# Réentraîner le modèle avec les caractéristiques réduites\n",
    "best_cat.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Supprimer les mêmes caractéristiques de la base test_data\n",
    "test_data_reduced = test_data2[important_features]\n",
    "\n",
    "# Prédire la variable cible pour la base test_data réduite (les variables explicatives)\n",
    "y_pred_test = best_cat.predict(test_data_reduced)\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "y_pred_one_hot = np.zeros((len(y_pred_test), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred_test):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (utiliser l'index de test_data comme identifiant)\n",
    "predictions_one_hot_df.insert(0, 'ID', test_data2.index)\n",
    "\n",
    "# Exporter les résultats dans un fichier CSV\n",
    "predictions_one_hot_df.to_csv('predictions4_catboost_best_one_hot.csv', index=False)\n",
    "\n",
    "print(\"Prédictions one-hot enregistrées dans le fichier 'predictions4_catboost_best_one_hot.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATBOOST avec une création de variables combinées pour essayer d'augmenter l'importance de variables qui ne contribuent\n",
    "# pas au modèle mais qui le devraient logiquement\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Créer de nouvelles caractéristiques dans test_data2 et train_data2 en multipliant les variables peu utile\n",
    "train_data2['TEAM_SHOTS_ON_TARGET_5_last_match_std_combined'] = train_data2['TEAM_SHOTS_ON_TARGET_5_last_match_std'] * train_data2['TEAM_DANGEROUS_ATTACKS_5_last_match_std'] * train_data2['TEAM_GOALS_5_last_match_std']\n",
    "train_data2['TEAM_DANGEROUS_ATTACKS_season_std_combined'] = train_data2['TEAM_DANGEROUS_ATTACKS_season_std'] * train_data2['TEAM_SHOTS_OUTSIDEBOX_season_std']\n",
    "\n",
    "test_data2['TEAM_SHOTS_ON_TARGET_5_last_match_std_combined'] = test_data2['TEAM_SHOTS_ON_TARGET_5_last_match_std'] * test_data2['TEAM_DANGEROUS_ATTACKS_5_last_match_std'] * test_data2['TEAM_GOALS_5_last_match_std']\n",
    "test_data2['TEAM_DANGEROUS_ATTACKS_season_std_combined'] = test_data2['TEAM_DANGEROUS_ATTACKS_season_std'] * test_data2['TEAM_SHOTS_OUTSIDEBOX_season_std']\n",
    "\n",
    "# Supprimer les anciennes colonnes pour éviter les doublons\n",
    "train_data2 = train_data2.drop(columns=['TEAM_SHOTS_ON_TARGET_5_last_match_std', 'TEAM_DANGEROUS_ATTACKS_5_last_match_std', 'TEAM_GOALS_5_last_match_std', 'TEAM_DANGEROUS_ATTACKS_season_std', 'TEAM_SHOTS_OUTSIDEBOX_season_std'])\n",
    "test_data2 = test_data2.drop(columns=['TEAM_SHOTS_ON_TARGET_5_last_match_std', 'TEAM_DANGEROUS_ATTACKS_5_last_match_std', 'TEAM_GOALS_5_last_match_std', 'TEAM_DANGEROUS_ATTACKS_season_std', 'TEAM_SHOTS_OUTSIDEBOX_season_std'])\n",
    "\n",
    "# Définir les données d'entraînement\n",
    "target_variable = 'target_variable' \n",
    "X_train = train_data2.drop(columns=[target_variable])\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "# Définir les hyperparamètres optimaux trouvés\n",
    "best_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'l2_leaf_reg': 15,\n",
    "    'iterations': 50,\n",
    "    'depth': 6,\n",
    "    'border_count': 128,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'random_seed': 64,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Créer le modèle CatBoost avec les meilleurs hyperparamètres\n",
    "best_cat = CatBoostClassifier(**best_params)\n",
    "\n",
    "# Entraîner le modèle avec les meilleurs hyperparamètres trouvés\n",
    "best_cat.fit(X_train, y_train)\n",
    "\n",
    "# Prédire la variable cible pour la base test_data (les variables explicatives)\n",
    "y_pred_test = best_cat.predict(test_data2)\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "y_pred_one_hot = np.zeros((len(y_pred_test), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred_test):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (utiliser l'index de test_data comme identifiant)\n",
    "predictions_one_hot_df.insert(0, 'ID', test_data2.index)\n",
    "\n",
    "# Exporter les résultats dans un fichier CSV\n",
    "predictions_one_hot_df.to_csv('predictionscombinee_catboost_best_one_hot.csv', index=False)\n",
    "\n",
    "print(\"Prédictions one-hot enregistrées dans le fichier 'predictionscombinee_catboost_best_one_hot.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST avec optuna \n",
    "target_variable = 'target_variable'  # Remplacer par le nom de ta variable cible\n",
    "X_train = train_data2.drop(columns=[target_variable])\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "# Diviser les données d'entraînement en ensemble d'entraînement et de validation\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train, y_train, test_size=0.2, random_state=64)\n",
    "\n",
    "# Fonction d'objectif pour l'optimisation d'Optuna\n",
    "def objective(trial):\n",
    "    # Définir l'espace de recherche des hyperparamètres\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 3,\n",
    "        'booster': 'gbtree',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.01, 0.5)\n",
    "    }\n",
    "\n",
    "    # Entraîner le modèle avec les paramètres proposés\n",
    "    model = xgb.XGBClassifier(**param, random_state=64)\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "\n",
    "    # Prédire sur l'ensemble de validation\n",
    "    y_pred_valid = model.predict(X_valid_split)\n",
    "\n",
    "    # Calculer la précision sur l'ensemble de validation\n",
    "    accuracy = accuracy_score(y_valid_split, y_pred_valid)\n",
    "    \n",
    "    # Retourner la précision (plus la valeur est élevée, mieux c'est)\n",
    "    return accuracy\n",
    "\n",
    "# Créer une étude Optuna et trouver les meilleurs hyperparamètres\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres trouvés\n",
    "best_params = study.best_params\n",
    "print(\"Meilleurs hyperparamètres trouvés :\")\n",
    "print(best_params)\n",
    "\n",
    "# Utiliser les meilleurs hyperparamètres trouvés pour entraîner le modèle final\n",
    "best_xgb = xgb.XGBClassifier(**best_params, objective='multi:softmax', num_class=3, random_state=64)\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Prédire la variable cible pour la base test_data (les variables explicatives)\n",
    "y_pred_test = best_xgb.predict(test_data2)\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "y_pred_one_hot = np.zeros((len(y_pred_test), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred_test):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (utiliser l'index de test_data comme identifiant)\n",
    "predictions_one_hot_df.insert(0, 'ID', test_data2.index)\n",
    "\n",
    "# Exporter les résultats dans un fichier CSV\n",
    "predictions_one_hot_df.to_csv('predictions2_xgboost_optuna_one_hot.csv', index=False)\n",
    "\n",
    "print(\"Prédictions one-hot enregistrées dans le fichier 'predictions2_xgboost_optuna_one_hot.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest \n",
    "\n",
    "seed = 64\n",
    "\n",
    "target_variable = 'target_variable'  \n",
    "X_train = train_data2.drop(columns=[target_variable])\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 200],  \n",
    "    'max_features': ['sqrt', 'log2'], \n",
    "    'max_depth': [5, 8, 10],  \n",
    "    'min_samples_split': [10, 15, 20],  \n",
    "    'min_samples_leaf': [4, 6, 8], \n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Utiliser RandomizedSearchCV pour trouver les meilleurs hyperparamètres\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
    "                               n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Entraîner à nouveau\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres trouvés :\")\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "# Prédire sur les données d'entraînement avec les meilleurs hyperparamètres trouvés\n",
    "best_rf = rf_random.best_estimator_\n",
    "y_pred_train = best_rf.predict(X_train)\n",
    "\n",
    "# Calculer et afficher la précision sur les données d'entraînement\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(f'Précision sur les données d\\'entraînement après ajustement : {accuracy_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest avec d'autres hyper paramètres pour tester\n",
    "\n",
    "seed = 64\n",
    "\n",
    "target_variable = 'target_variable'  # Remplace par le nom de ta variable cible\n",
    "X_train = train_data2.drop(columns=[target_variable])\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 150, 125, 75],\n",
    "    'max_features': ['sqrt', 'log2', 0.4, 0.3, 0.5, 0.2],  \n",
    "    'max_depth': [7, 8, 9, 10, 11, 12, 15, 20], \n",
    "    'min_samples_split': [8, 9, 10, 12, 14, 11, 13], \n",
    "    'min_samples_leaf': [4, 5, 6, 7, 8, 9, 10],  \n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Utiliser RandomizedSearchCV pour trouver les meilleurs hyperparamètres\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
    "                               n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Entraîner à nouveau\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres trouvés :\")\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "# Prédire sur les données d'entraînement avec les meilleurs hyperparamètres trouvés\n",
    "best_rf2 = rf_random.best_estimator_\n",
    "y_pred_train = best_rf2.predict(X_train)\n",
    "\n",
    "# Calculer et afficher la précision sur les données d'entraînement\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(f'Précision sur les données d\\'entraînement après ajustement : {accuracy_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation croisée pour comparer les modèles entre eux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer une validation croisée sur le modèle optimisé\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Calculer la précision moyenne et l'écart-type\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "\n",
    "print(f'Précision moyenne avec validation croisée à 5 folds : {cv_mean:.4f}')\n",
    "print(f'Écart-type des précisions : {cv_std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accéder à l'importance des variables dans le modèle entraîné\n",
    "feature_importances = best_rf2.feature_importances_\n",
    "\n",
    "# Créer un DataFrame pour associer les importances aux noms des caractéristiques\n",
    "features = X_train.columns \n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "# Trier les variables par ordre décroissant d'importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Afficher les variables les plus importantes\n",
    "print(importance_df)\n",
    "\n",
    "# Visualiser l'importance des variables avec un graphique en barres\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['Feature'][:15], importance_df['Importance'][:15], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 15 Variables les Plus Importantes')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de random forest en mettant un seuil d'importance des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir un seuil pour l'importance des variables\n",
    "importance_threshold = 0.00955\n",
    "\n",
    "# Identifier les variables importantes\n",
    "important_features = importance_df[importance_df['Importance'] > importance_threshold]['Feature'].tolist()\n",
    "\n",
    "# Afficher les variables à conserver\n",
    "print(\"Variables conservées :\", important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre à jour les jeux de données avec seulement les variables importantes\n",
    "X_train_filtered = X_train[important_features]\n",
    "test_data_filtered = test_data2[important_features]\n",
    "\n",
    "# Afficher la nouvelle forme des jeux de données\n",
    "print(\"Nouveau nombre de variables :\", X_train_filtered.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seed = 64\n",
    "\n",
    "# Définir les données d'entraînement avec les variables restantes\n",
    "target_variable = 'target_variable'  \n",
    "X_train_filtered = X_train[important_features]  # Utiliser uniquement les variables restantes\n",
    "y_train = train_data2[target_variable]\n",
    "\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 150, 125, 75],  \n",
    "    'max_features': ['sqrt', 'log2', 0.4, 0.3, 0.5, 0.2],  \n",
    "    'max_depth': [7, 8, 9, 10, 11, 12, 15, 20],  \n",
    "    'min_samples_split': [8, 9, 10, 12, 14, 11, 13],  \n",
    "    'min_samples_leaf': [4, 5, 6, 7, 8, 9, 10], \n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Utiliser RandomizedSearchCV pour trouver les meilleurs hyperparamètres\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
    "                               n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Entraîner à nouveau\n",
    "rf_random.fit(X_train_filtered, y_train)\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres trouvés :\")\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "# Prédire sur les données d'entraînement avec les meilleurs hyperparamètres trouvés\n",
    "best_rf2 = rf_random.best_estimator_\n",
    "y_pred_train = best_rf2.predict(X_train_filtered)\n",
    "\n",
    "# Prédire la variable cible pour la base test_data (les variables explicatives)\n",
    "y_pred_test = best_rf2.predict(test_data2[important_features])\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "y_pred_one_hot = np.zeros((len(y_pred_test), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred_test):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (utiliser l'index de test_data comme identifiant)\n",
    "predictions_one_hot_df.insert(0, 'ID', test_data2.index)\n",
    "\n",
    "# Exporter les résultats dans un fichier CSV\n",
    "predictions_one_hot_df.to_csv('predictions2_rf_best_one_hot.csv', index=False)\n",
    "\n",
    "print(\"Prédictions one-hot enregistrées dans le fichier 'predictions2_rf_best_one_hot.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = best_rf2.predict(test_data2)\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "# On suppose que test_data a une colonne d'ID ou d'index pour identifier chaque prédiction\n",
    "predictions = pd.DataFrame({\n",
    "    'ID': test_data2.index,  \n",
    "    'target_variable': y_pred_test\n",
    "})\n",
    "\n",
    "# Exporter les résultats dans un fichier CSV\n",
    "predictions.to_csv('predictions2.csv', index=False)\n",
    "\n",
    "print(\"Prédictions enregistrées dans le fichier 'predictions2.csv'\")\n",
    "\n",
    "# Charger les prédictions depuis le fichier CSV 'y_pred_test'\n",
    "y_pred_df = pd.read_csv('predictions2.csv')\n",
    "\n",
    "# Assumer que le fichier CSV contient une colonne 'target_variable' qui a les valeurs prédictes (0, 1, 2)\n",
    "y_pred = y_pred_df['target_variable']\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "# Chaque ligne représente un match, chaque colonne représente une classe : [HOME_WINS, DRAW, AWAY_WINS]\n",
    "y_pred_one_hot = np.zeros((len(y_pred), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions2_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (si nécessaire, utiliser l'index du fichier initial)\n",
    "if 'ID' in y_pred_df.columns:\n",
    "    predictions2_one_hot_df.insert(0, 'ID', y_pred_df['ID'])\n",
    "else:\n",
    "    predictions2_one_hot_df.insert(0, 'ID', y_pred_df.index)\n",
    "\n",
    "# Exporter les résultats dans un nouveau fichier CSV\n",
    "predictions2_one_hot_df.to_csv('predictions2_one_hot_df.csv', index=False)\n",
    "\n",
    "print(\"Prédictions one-hot enregistrées dans le fichier 'predictions2_one_hot_df.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test d'un light GBM\n",
    "\n",
    "# Définir une valeur de seed pour la reproductibilité\n",
    "seed = 64\n",
    "\n",
    "# Charger et entraîner le modèle LightGBM\n",
    "model = lgb.LGBMClassifier(objective='multiclass', num_class=3, random_state=64)\n",
    "model.fit(X_train, y_train)  # Entraîner le modèle sur les données d'entraînement\n",
    "\n",
    "# Prédire la variable cible pour la base test_data (les variables explicatives)\n",
    "y_pred_test = model.predict(test_data2)\n",
    "\n",
    "# Créer une matrice de zéros pour les prédictions one-hot\n",
    "# Chaque ligne représente un match, chaque colonne représente une classe : [HOME_WINS, DRAW, AWAY_WINS]\n",
    "y_pred_one_hot = np.zeros((len(y_pred_test), 3), dtype=int)\n",
    "\n",
    "# Remplir la matrice one-hot en fonction des prédictions\n",
    "for i, pred in enumerate(y_pred_test):\n",
    "    y_pred_one_hot[i, pred] = 1\n",
    "\n",
    "# Créer un DataFrame pour les résultats de prédiction\n",
    "predictions3_one_hot_df = pd.DataFrame(y_pred_one_hot, columns=['HOME_WINS', 'DRAW', 'AWAY_WINS'])\n",
    "\n",
    "# Ajouter une colonne ID (si nécessaire, utiliser l'index du fichier test_data comme identifiant)\n",
    "predictions3_one_hot_df.insert(0, 'ID', test_data2.index)\n",
    "\n",
    "# Exporter les résultats dans un nouveau fichier CSV avec des entiers\n",
    "predictions3_one_hot_df.to_csv('predictions2_lightgbm_one_hot.csv', index=False)\n",
    "\n",
    "print(\"Prédictions one-hot enregistrées dans le fichier 'predictions2_lightgbm_one_hot.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
